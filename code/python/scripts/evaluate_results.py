import argparse
import csv
from rouge import Rouge 
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from bert_score import score
from pprint import pprint



def load_csv_with_answers(filepath):
    with open(filepath, 'r') as file:
        reader = csv.DictReader(file)  # use DictReader to read by column names
        data = [(row['instruction'],row['output'], row['answer']) for row in reader]
    return data


from bert_score import score as bert_score

def evaluate_responses(responses, references):
    rouge_evaluator = Rouge()
    # Initialize lists to store scores for each observation
    rouge_scores_list = []
    bleu_scores_list = []
    bert_precisions = []
    bert_recalls = []
    bert_f1_scores = []

    smoothie = SmoothingFunction().method4

    # Calculate scores per observation
    for response, reference in zip(responses, references):
        # Compute ROUGE scores
        rouge_scores = rouge_evaluator.get_scores(response, reference)
        rouge_scores_list.append(rouge_scores[0])  # Get the first (and only) score

        # Compute BLEU score
        bleu_score = sentence_bleu([reference.split()], response.split(), smoothing_function=smoothie)
        bleu_scores_list.append(bleu_score)

    # Compute BERT scores for all responses at once for efficiency
    P, R, F1 = bert_score(responses, references, lang='en', verbose=True)
    bert_precisions.extend(P)
    bert_recalls.extend(R)
    bert_f1_scores.extend(F1)

    # Combine all scores into a single list of dictionaries with wide format
    observation_scores = []
    for idx in range(len(responses)):
        # Extracting ROUGE scores
        rouge_1 = rouge_scores_list[idx]['rouge-1']['f']
        rouge_2 = rouge_scores_list[idx]['rouge-2']['f']
        rouge_l = rouge_scores_list[idx]['rouge-l']['f']

        observation_scores.append({
            "ID": idx,
            "BERT_Precision": bert_precisions[idx].item(),
            "BERT_Recall": bert_recalls[idx].item(),
            "BERT_F1": bert_f1_scores[idx].item(),
            "ROUGE-1_F1": rouge_1,
            "ROUGE-2_F1": rouge_2,
            "ROUGE-L_F1": rouge_l,
            "BLEU": bleu_scores_list[idx],
        })

    return observation_scores


# Write scores to CSV

def write_scores_to_csv(scores, output_path):
    # Define the CSV file's header using the keys of the first score dictionary
    fieldnames = list(scores[0].keys())
    
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        # Write the header
        writer.writeheader()
        
        # Write each score observation in rows
        for score in scores:
            writer.writerow(score)


   
if __name__ == "__main__":
    # Argument parser setup
    parser = argparse.ArgumentParser(description="Generate text using a given model.")
    parser.add_argument("--csv_path", type=str, required=False, help="Path to the CSV file with questions and answers.")
    parser.add_argument("--output_csv", type=str, default=None, help="Path to save the model's answers in CSV format.")
   

    args = parser.parse_args()

    print(f"[] Calculating metrics")
    print(f"[] Loading file: {args.csv_path}")
    # Load the evaluation CSV data
    eval_data = load_csv_with_answers(args.csv_path)
    print(eval_data[0])
    eval_answers = [row[2] for row in eval_data]  # Assuming that the answers are in the third column
    eval_responses = [row[1] for row in eval_data]  # Assuming that the responses are in the second column
    
    # Evaluate the loaded responses and answers
    eval_scores = evaluate_responses(eval_responses, eval_answers)
    write_scores_to_csv(eval_scores,"caca.csv")


    # If an output CSV path is specified, write the answers to it
    if args.output_csv:
         write_scores_to_csv(eval_scores,args.output_csv)
       