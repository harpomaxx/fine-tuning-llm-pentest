import argparse
import torch

from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)
from transformers import pipeline, set_seed
from transformers import pipeline
from pynvml import *
import csv
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge import Rouge 
from bert_score import score
from pprint import pprint
from tqdm import tqdm

def load_csv(filepath):
    with open(filepath, 'r') as file:
        reader = csv.DictReader(file)  # use DictReader to read by column names
        data = [(row['ID'],row['Question'], row['Answer']) for row in reader]
    return data

def evaluate_responses(responses, references):
    rouge = Rouge()
    rouge_scores = rouge.get_scores(responses, references, avg=True)
    # Calculate BLEU score
    bleu_scores = []
    smoothie = SmoothingFunction().method4  # This is a smoothing method. There are several available.

    for response, reference in zip(responses, references):
        bleu_score = sentence_bleu([reference.split()], response.split(), smoothing_function=smoothie)
        bleu_scores.append(bleu_score)

    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)

      # Calculate BERTScore
    P, R, F1 = score(responses, references, lang='en', verbose=True)
    avg_F1 = F1.mean().item()


    # Return both ROUGE and BLEU and BERT scores
    return {
        "BERT": avg_F1,
        "ROUGE": rouge_scores,
        "BLEU": avg_bleu_score
    }
    return scores   

def generate_text(prompt, generator, template):
    formatted_prompt = template.format(prompt=prompt)
    llm_ans =generator(formatted_prompt)
    return llm_ans[0]['generated_text']

def print_gpu_utilization():
    nvmlInit()
    handle = nvmlDeviceGetHandleByIndex(0)
    info = nvmlDeviceGetMemoryInfo(handle)
    print(f"GPU memory occupied: {info.used//1024**2} MB.")

def write_to_csv(output_path, ids, answers):
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        csvwriter = csv.writer(csvfile)  
        # Write header
        csvwriter.writerow(["ID", "Model Answer"])
        # Write data
        for idx, answer in zip(ids, answers):
            csvwriter.writerow([idx, answer])
   
if __name__ == "__main__":
    # Argument parser setup
    parser = argparse.ArgumentParser(description="Generate text using a given model.")
    parser.add_argument("--model_path", type=str, default="facebook/opt-350m", help="Path to the pretrained model.")
    parser.add_argument("--tokenizer_path", type=str, default="facebook/opt-350m", help="Path to the pretrained tokenizer.")
    parser.add_argument("--quantize", action="store_true", default = False, help="Load the model using quantization.")
    parser.add_argument("--template", type=str, default="### Human {prompt} ### Assistant", help="Template for the prompt. Eg. ### Human: {prompt} ### Assistant: ")
    parser.add_argument("--csv_path", type=str, required=True, help="Path to the CSV file with questions and answers.")
    parser.add_argument("--output_csv", type=str, default=None, help="Path to save the model's answers in CSV format.")


    args = parser.parse_args()

    print(f"[] Selected Model Path: {args.model_path}")
    print(f"[] Selected Tokenizer Path: {args.tokenizer_path}")
    print(f"[] Using Quantization: {'Yes' if args.quantize else 'No'}")
    print_gpu_utilization()
    bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype= getattr(torch, "float16"),
    bnb_4bit_use_double_quant=True,
    )
   
    quantization_config = bnb_config if args.quantize else None


    model_code = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            quantization_config=quantization_config,
            device_map ="auto",
            trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, trust_remote_code=True)

    set_seed(32)
    print_gpu_utilization()
    generator = pipeline('text-generation', 
                         model=model_code, 
                         tokenizer=tokenizer, 
                         do_sample=True,
                         max_length=256,
                         return_full_text=False)
    

    csv_data = load_csv(args.csv_path)
    csv_answers = []
    model_responses = []
    csv_ids = []  # To collect the IDs from your CSV data

    for csv_id, question, csv_answer in tqdm(csv_data, desc="Processing questions"):
        model_response = generate_text(question, generator, args.template)
        model_responses.append(model_response)
        csv_ids.append(csv_id)  # Collect the ID
        csv_answers.append(csv_answer)

    scores = evaluate_responses(model_responses, csv_answers)
    pprint(scores)

    # If an output CSV path is specified, write the answers to it
    if args.output_csv:
        write_to_csv(args.output_csv, csv_ids, model_responses)
