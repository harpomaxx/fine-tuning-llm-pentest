# Inference.py 

`inference.py` is a command-line utility script for generating text using pre-trained models. It offers flexibility through various parameters, including the use of quantized models, custom templates for prompts, and more. Additionally, the script provides insights into GPU memory utilization.

## Features:
1. Generate text using provided or default prompts.
2. Utilize quantized models for inference.
3. Display GPU memory utilization.
4. Interactive CLI mode for user prompts.
5. Use custom templates for prompts.

## Requirements:
- `torch`
- `transformers`
- `pynvml`

## Usage:

### Basic Usage:
Generate text with default settings.
```bash
python inference.py
```

### Advanced Usage:

Generate text with a custom model, tokenizer, and prompt, using quantization in interactive CLI mode:
```bash
python inference.py --model_path=path/to/model --tokenizer_path=path/to/tokenizer --prompt="Your custom prompt here" --quantize --cli
```

Generate text with a different template:
```bash
python inference.py --template="User Question: {prompt} || AI Response:"
```

### Output:
The script will:
1. Print selected configurations.
2. Display GPU memory utilization before and after model loading.
3. Generate the response to the provided or default prompt.

### Note:
Ensure GPU drivers and `pynvml` are set up correctly to retrieve GPU utilization information.

---

**Author**: Harpo MAxx
**License**: MIT 

---
