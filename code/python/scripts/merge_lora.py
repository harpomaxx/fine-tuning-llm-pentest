import torch
from datasets import load_dataset, Dataset
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)
from trl import SFTTrainer,DataCollatorForCompletionOnlyLM
from sklearn.model_selection import train_test_split
from pynvml import *
import pandas as pd
import random
import mlflow
import os
os.environ["WANDB_DISABLED"] = "true"


## CONFIGURATION

model_path = "/data/harpo/llm-models/Llama-2-7b-hf"
tokenizer_path = "/data/harpo/llm-models/Llama-2-7b-hf"
trained_model_checkpoints_save_path = "/data/harpo/llm-models/checkpoints/"
trained_model_save_path = "/data/harpo/llm-models/Llama-2-7b-hf-guanaco"

# ### Bits and Bytes configuration for using quantization
compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        #device_map={"": 0}, # for setting a particular GPU
        device_map="auto", # for multiple GPUs
        trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, 
                                          trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token # Required for LLama-2 models

model = PeftModel.from_pretrained(model,f'{trained_model_save_path}/lora',local_files_only=True)
#merged_model = model.merge_and_unload() # Not working with 8bit models
print(f"[] Saving Model at {trained_model_save_path}")
model.save_pretrained(trained_model_save_path,safe_serialization=True)
print(f"[] Saving Tokenizer at {trained_model_save_path}")
tokenizer.save_pretrained(trained_model_save_path,safe_serialization=True)