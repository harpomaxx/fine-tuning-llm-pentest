{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning OPT350m on a single GPU\n",
    "\n",
    "**GOAL:** fine tune opt350m for code assistance \n",
    "\n",
    "**Method:**  train on response only using code-alpaca-20k dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpo/miniconda3/envs/fastchat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from trl import SFTTrainer,DataCollatorForCompletionOnlyLM\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.28.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions for analyzing the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 237 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and Bytes configuration for using quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"facebook/opt-350m\",\n",
    "        #quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and write the code to define the first four number elements a given number of times, or the definition of the last 4 number elements a given number of times (or the third number element is an infinite number of times). The code can be used to compute the prime numbers independently for arbitrary values of the number; for example, if the function returns a single prime number, the calculation is performed by using the value of the first four number element of the first parameter alone, and only two prime numbers are required for the computation (i.e., the first 4 number element).\n",
      "In one preferred embodiment, the function to calculate prime numbers may be a function to determine how many prime numbers a given number of times are included in a set of prime numbers. This function also preferably has the capability of calculating the prime numbers and prime numbers in successive numbers, and calculating the prime numbers and prime numbers in successive numbers independently of each other.\n",
      "In another preferred embodiment, the function to calculate prime numbers may first determine what prime number number and prime number are included in each parameter of the function, and then calculate one prime number and prime number independently. The function may then further determine where in the table the prime numbers are located. In this way,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import TextStreamer, pipeline\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "set_seed(32)\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model, \n",
    "                     tokenizer=tokenizer, \n",
    "                     streamer = streamer, \n",
    "                     do_sample=True,\n",
    "                     max_length= 256)\n",
    "prompt = \"\"\"Code a function for calculating prime numbers\"\"\"\n",
    "_=generator(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRa Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 335,915,008 || trainable%: 1.4046981788917272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.5,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\"] # obtained by the output of the model\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../../../results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    optim='adamw_bnb_8bit',\n",
    "    save_steps=250,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=0.3,\n",
    "    #max_steps=5000,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    ymebptcmbpojhagr.com\n",
       "2    lucdkkoqvhinwgod.com\n",
       "3    olndrknyjxbdemdv.com\n",
       "4    gjemhdldbagnklvf.com\n",
       "5    kftmoafohjffohtk.com\n",
       "6    emynldrjxpdcwwrj.com\n",
       "7    vpccwfvbdukouykn.com\n",
       "8    delhcrpkmhrgkcmo.com\n",
       "9    prvaywqbagdkhdoa.com\n",
       "Name: domain, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "#train_dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "import pandas as pd\n",
    "train_dataset = pd.read_csv(\"../../../data/domains_finetuning_binary_v2.csv\")\n",
    "train_dataset[1:10]['domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 10",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdomain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m train_dataset[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdomain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m train_dataset[\u001b[39m10\u001b[39;49m]\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "train_dataset[:-1]['instruction'][1]\n",
    "train_dataset[:-1]['instruction'][1]\n",
    "\n",
    "train_dataset[10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formating function for codealpaca\n",
    "\n",
    "The training is done in a way similar to llama-2 [8] paper were loss is only calculated on the answer (completion). However, do to HF's `trl` limitations, no packing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorForCompletionOnlyLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m output_texts\n\u001b[1;32m      8\u001b[0m response_template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m ### Answer:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m collator \u001b[39m=\u001b[39m DataCollatorForCompletionOnlyLM(response_template, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCollatorForCompletionOnlyLM' is not defined"
     ]
    }
   ],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['domain'])):\n",
    "        text = f\"### Question: {example['domain'][i]}\\n ### Answer: {example['family'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"### Question: Generate a unique 8 character string that contains a lowercase letter, an uppercase letter, a numerical digit, and a special character. Write corresponding code in Python.\\n ### Answer: import string\\nimport random\\n\\ndef random_password_string():\\n    characters = string.ascii_letters + string.digits + string.punctuation\\n    password = ''.join(random.sample(characters, 8))\\n    return password\\n\\nif __name__ == '__main__':\\n    print(random_password_string())\",\n",
       " '### Question: Given a course consisting of 5 lessons, write a program to display the first lesson. Write corresponding code in Python.\\n ### Answer: def display_first_lesson(lessons):\\n    print(\"The first lesson is: \" + lessons[0])\\n\\nLessons = [\"Python Basics\", \"Javascript Basics\", \"C++ Basics\", \"Data Structures\", \"Object-Oriented Programming\"]\\n\\ndisplay_first_lesson(Lessons)',\n",
       " \"### Question: Create an algorithm to encourage work balance and productivity in remote work.\\n ### Answer: One algorithm to encourage work balance and productivity in remote work is a scheduling algorithm that assigns workloads over manageable working windows with rest periods in between to allow for breaks and recovery. This algorithm can be designed to set maximum and minimum limit for each allocated workload and for duration for the rest periods, based on the individual's productive and rest needs. It can be used to monitor and manage workloads to ensure that workers do not experience fatigue or workload overload. Additionally, the algorithm can be designed to keep track of work performance over time to provide appropriate feedback and recommendations in order to help workers optimize their performance.\",\n",
       " '### Question: Write a JavaScript that changes the text of a paragraph element, from \"Hello, World!\" to \"Goodbye, World!\"\\n ### Answer: document.getElementById(\"myParagraph\").innerHTML = \"Goodbye, World!\";',\n",
       " '### Question: Implement a sorting algorithm which takes an array of integers as input and sorts it in ascending order.\\n ### Answer: def sorting_algorithm(arr):\\n    for i in range(len(arr)-1):\\n        for j in range(i+1, len(arr)):\\n            if arr[i] > arr[j]:\\n                arr[i], arr[j] = arr[j], arr[i]\\n    return arr\\n\\n# Test\\narr = [34, 19, 42, -9, 2018, 0, 105]\\nprint(sorting_algorithm(arr)) # [-9, 0, 19, 34, 42, 105, 2018]',\n",
       " '### Question: Generate a C code snippet to print a given string with a width of 20 characters. Write corresponding code in Python.\\n ### Answer: #include <stdio.h> \\n#include <string.h> \\n  \\nint main(){ \\n    char str[20] = \"Hello\"; \\n    printf(\"%-20s\\\\n\", str); \\n    return 0;\\n}',\n",
       " '### Question: Construct a loop in Swift to find duplicate numbers in an array. Write corresponding code in Python.\\n ### Answer: func FindDuplicates(arr: [Int])  -> [Int] {\\n\\tvar seenNumbers = Set<Int>()\\n\\tvar duplicateValues = [Int]()\\n \\n\\tfor number in arr {\\n\\t\\tif seenNumbers.contains(number) {\\n\\t\\t\\tduplicateValues.append(number)\\n\\t\\t} else {\\n\\t\\t\\tseenNumbers.insert(number)\\n\\t\\t}\\n\\t}\\n \\n\\treturn duplicateValues\\n}',\n",
       " '### Question: Use an object oriented approach to create a class in Java to store the cost and name of an item.\\n ### Answer: class Item { \\n    private String name; \\n    private double cost;\\n \\n    // ... setter and getter methods\\n}',\n",
       " '### Question: Write a Java program that passes an array to a method and prints its length.\\n ### Answer: public class TestArrayLength {\\n    public static void printLength(int[] array) {\\n        System.out.println(\"The length of the array is: \" + array.length);\\n    }\\n    public static void main(String[] args) {\\n        int[] numbers = {1, 2, 3, 4, 5};\\n        printLength(numbers);\\n    }\\n}']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_examples = formatting_prompts_func(train_dataset[1:10])\n",
    "some_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForCompletionOnlyLM(tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"opt350m-codealpaca-20k2\"\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    #dataset_text_field=\"text\",\n",
    "    formatting_func= formatting_prompts_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    data_collator= collator\n",
    ")\n",
    "\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpo/miniconda3/envs/fastchat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import TextStreamer, pipeline\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"facebook/opt-350m\",\n",
    "        #\"/home/harpo/CEPH/LLM-models/opt350-dga/\",\n",
    "       # quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "# You can comment and un comment this line to either use base model \n",
    "# or the peft model during the inference.\n",
    "model = PeftModel.from_pretrained(model,'/home/harpo/CEPH/LLM-models/opt350-pentest/lora/',local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "set_seed(32)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model, \n",
    "                     tokenizer=tokenizer, \n",
    "                     streamer = streamer, \n",
    "                     do_sample=True,\n",
    "                     temperature=0.1,\n",
    "                     max_length= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpo/miniconda3/envs/fastchat/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 18, but `max_length` is set to 2. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"#domain: www.losandes.com.ar\\n#label: \"\"\"\n",
    "_=generator(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for calculting the probability of a given token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_token_probability(model, tokenizer, context, target_token):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(context, return_tensors='pt').to(\"cuda\")\n",
    "    \n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, -1, :]  # get the logits of the last token in the input\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the index of the target token\n",
    "    token_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Get the probability of the target token\n",
    "    target_probability = probabilities[0, token_id].item()\n",
    "    \n",
    "    return target_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the token 'pos' given the context '#domain: www.losandes.com.ar\n",
      "#label: ' is: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the context and the target token\n",
    "context = \"\"\"#domain: www.losandes.com.ar\\n#label: \"\"\"\n",
    "target_token = \"pos\"\n",
    "\n",
    "# Get the probability\n",
    "probability = get_token_probability(model, tokenizer, context, target_token)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The probability of the token '{target_token}' given the context '{context}' is: {probability:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "demo = gr.Interface.from_pipeline(generator)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"/home/harpo/CEPH/LLM-models/opt350-pentest/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpo/miniconda3/envs/fastchat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import TextStreamer, pipeline\n",
    "\n",
    "model_code = AutoModelForCausalLM.from_pretrained(\n",
    "        \"/home/harpo/CEPH/LLM-models/opt350-openassistant-val\",\n",
    "        local_files_only= True,\n",
    "        #quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(32)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model_code, \n",
    "                     tokenizer=tokenizer, \n",
    "                     #streamer = streamer, \n",
    "                     do_sample=True,\n",
    "                     max_length= 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=model_code.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OPTForCausalLM' object has no attribute 'stream_chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m\u001b[39m.\u001b[39;49mstream_chat()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastchat/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OPTForCausalLM' object has no attribute 'stream_chat'"
     ]
    }
   ],
   "source": [
    "m.stream_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "$time = $time(2.5 * $x^2$) + 1\n",
      "if $x> 0:\n",
      "# # # > # # # #... #\n",
      "# # \"A\" > \"A\" * \"B\" + \"C\" * \"D\"\n",
      "else:\n",
      "# # # \"B\" > \"B\" * \"C\" * \"D\"\n",
      "# #\n",
      "# \"A\"> \"A\" * \"B\" - \"C\" * \"D\"\n",
      "else:\n",
      "# # # \"C\" > \"C\" * \"D\"\n",
      "\n",
      "Here are a few general instructions you can use to create various mathematical formulas for solving the prime numbers:\n",
      "1. Consider the sum of numbers of consecutive quadratic equations for the prime numbers and then multiply it by the average prime number:\n",
      "\n",
      "2. Consider the inverse of the prime number $x^2$ for the prime numbers:\n",
      "\n",
      "3. Use $x < $x^2 to compute the inverse of the prime number, $x^2 = $\\frac{x}$.\n",
      "4. Compare the values of $\\frac{x}{x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Code a function in PYTHON language for calculating prime numbers ### Answer:\\n\\n$time = $time(2.5 * $x^2$) + 1\\nif $x> 0:\\n# # # > # # # #... #\\n# # \"A\" > \"A\" * \"B\" + \"C\" * \"D\"\\nelse:\\n# # # \"B\" > \"B\" * \"C\" * \"D\"\\n# #\\n# \"A\"> \"A\" * \"B\" - \"C\" * \"D\"\\nelse:\\n# # # \"C\" > \"C\" * \"D\"\\n\\nHere are a few general instructions you can use to create various mathematical formulas for solving the prime numbers:\\n1. Consider the sum of numbers of consecutive quadratic equations for the prime numbers and then multiply it by the average prime number:\\n\\n2. Consider the inverse of the prime number $x^2$ for the prime numbers:\\n\\n3. Use $x < $x^2 to compute the inverse of the prime number, $x^2 = $\\\\frac{x}$.\\n4. Compare the values of $\\\\frac{x}{x'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Code a function in PYTHON language for calculating prime numbers ### Answer:\"\"\"\n",
    "a=generator(prompt)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface.from_pipeline(generator)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer)\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import torch\n",
    "\n",
    "model_code = AutoModelForCausalLM.from_pretrained(\n",
    "        \"harpomaxx/opt350m-codealpaca20k\",\n",
    "        local_files_only= True,\n",
    "        #quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = 0 if torch.cuda.is_available()==False else torch.cuda.device_count()\n",
    "model_code.to(device)\n",
    "\n",
    "\n",
    "def generate(text = \"\", max_new_tokens = 128, temperature = 0.1):  \n",
    "  streamer = TextIteratorStreamer(tok, skip_prompt=True, timeout=10.)\n",
    "  if len(text) == 0:\n",
    "    text = \" \"\n",
    "  inputs = tok([text], return_tensors=\"pt\").to(device)\n",
    "  generation_kwargs = dict(inputs, streamer=streamer, \n",
    "                           repetition_penalty=1.0, \n",
    "                           do_sample=True, \n",
    "                           top_k=40, \n",
    "                           top_p=0.97, \n",
    "                           max_new_tokens=max_new_tokens, \n",
    "                           #pad_token_id = model.config.eos_token_id, \n",
    "                           early_stopping=True, \n",
    "                           no_repeat_ngram_size=4,\n",
    "                           temperature = temperature * 1.0/100)\n",
    "  thread = Thread(target=model_code.generate, kwargs=generation_kwargs)\n",
    "  thread.start()\n",
    "  generated_text = \"\"\n",
    "  for new_text in streamer:\n",
    "    yield generated_text + new_text    \n",
    "    generated_text += new_text\n",
    "    if tok.eos_token in generated_text:\n",
    "      generated_text = generated_text[: generated_text.find(tok.eos_token) if tok.eos_token else None]\n",
    "      streamer.end()\n",
    "      yield generated_text\n",
    "      return\n",
    "  return generated_text\n",
    "\n",
    "# Gradio UI components and layout\n",
    "with gr.Blocks() as demo:\n",
    "    examples = [\n",
    "        [\"Once upon a time in a world where AI\", 2, 10],\n",
    "        [\"The future of AI lies in\", 1, 15],\n",
    "        [\"Deep in the heart of the forest\", 3, 5]\n",
    "    ]\n",
    "\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Text Generation Demo\n",
    "\n",
    "    ## Enter your prompt and generate text based on your input.\n",
    "    \"\"\"\n",
    "    )\n",
    "with gr.Column(variant=\"panel\"):\n",
    "    with gr.Row(variant=\"compact\"):\n",
    "        text = gr.Textbox(\n",
    "            label=\"Enter your prompt\",\n",
    "            show_label=True,\n",
    "            lines=20\n",
    "        ).style(\n",
    "            container=False)\n",
    "        \n",
    "        output_text = gr.Textbox(\n",
    "            readonly=False, \n",
    "            label=\"Generated Text\", \n",
    "            lines=20,\n",
    "            live=True,\n",
    "            interactive=False\n",
    "        ).style(\n",
    "            container=False)\n",
    "        \n",
    "    btn = gr.Button(\"Generate Text\").style(full_width=True)\n",
    "      \n",
    "    with gr.Row(variant=\"compact\"):\n",
    "        max_length_slider = gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=1024,\n",
    "            step=1,\n",
    "            value=512,\n",
    "            label=\"Max Length\",\n",
    "        )\n",
    "\n",
    "        temperature_slider = gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=100,\n",
    "            step=1,\n",
    "            value=20,\n",
    "            label=\"Temperature\",\n",
    "        )\n",
    "        \n",
    "        repetition_penalty_slider = gr.Slider(\n",
    "            minimum=0.5,\n",
    "            maximum=2.0,\n",
    "            step=0.1,\n",
    "            value=1.0,\n",
    "            label=\"Repetition Penalty\",\n",
    "        )\n",
    "        \n",
    "        do_sample_checkbox = gr.Checkbox(\n",
    "            default=True,\n",
    "            label=\"Do Sample\"\n",
    "        )\n",
    "        \n",
    "        top_k_slider = gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=100,\n",
    "            step=1,\n",
    "            value=40,\n",
    "            label=\"Top K\"\n",
    "        )\n",
    "\n",
    "        top_p_slider = gr.Slider(\n",
    "            minimum=0.0,\n",
    "            maximum=1.0,\n",
    "            step=0.01,\n",
    "            value=0.97,\n",
    "            label=\"Top P\"\n",
    "        )\n",
    "        \n",
    "        early_stopping_checkbox = gr.Checkbox(\n",
    "            default=True,\n",
    "            label=\"Early Stopping\"\n",
    "        )\n",
    "\n",
    "        no_repeat_ngram_size_slider = gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=10,\n",
    "            step=1,\n",
    "            value=4,\n",
    "            label=\"No Repeat N-Gram Size\"\n",
    "        )\n",
    "  \n",
    "\n",
    "   \n",
    "    # Display generated text\n",
    "      \n",
    "btn.click(generate, \n",
    "          [text, max_length_slider, temperature_slider, repetition_penalty_slider, do_sample_checkbox, top_k_slider, top_p_slider, early_stopping_checkbox, no_repeat_ngram_size_slider], \n",
    "          output_text, show_progress=False)\n",
    "\n",
    "    #gr.Examples(examples, inputs=[text, max_length_slider, temperature_slider])\n",
    "    \n",
    "    \n",
    "demo.queue(concurrency_count=5,max_size=20)\n",
    "demo.launch(share = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "\n",
    "[1] https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91\n",
    "\n",
    "[2] https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only\n",
    "\n",
    "[3] https://huggingface.co/facebook/opt-350m\n",
    "\n",
    "[4] https://medium.com/@rohit.pegallapati/fine-tune-falcon-7b-instruct-model-on-single-commodity-gpu-cf65a86c043a\n",
    "\n",
    "[5] https://huggingface.co/docs/transformers/v4.23.1/en/perf_train_gpu_one\n",
    "\n",
    "[6] [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)\n",
    "\n",
    "[7] https://huggingface.co/docs/peft/conceptual_guides/\n",
    "\n",
    "[8] [Llama 2 paper](https://arxiv.org/pdf/2307.09288.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
